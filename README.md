# Distill
Create a mathematical reasoning model through distillation.


<img width="512" height="512" alt="unnamed" src="https://github.com/user-attachments/assets/f9965896-2f3c-44a5-bc78-70a574fb3fd8" />

<img width="400" height="180" alt="poop" src="https://github.com/user-attachments/assets/921e7016-c14f-4396-ba9f-a8b27d200d67" />

<p>&nbsp;</p>  

Model distillation is a process where a large ***teacher*** model trains a smaller ***student*** model to replicate its behavior, allowing the student to achieve comparable performance with far fewer parameters. By transferring the teacher’s knowledge, the student retains much of the accuracy while being faster, cheaper, and easier to deploy.

Distilled models can be trained to ***specialize*** in particular tasks or domains such as science, coding, mathematics, and information extraction by inheriting the teacher’s expertise in those areas while remaining compact and efficient for on-prem deployment.

<p>&nbsp;</p>

<img width="768" height="319" alt="knowledge-distillation copy" src="https://github.com/user-attachments/assets/bec18a80-6390-41ff-aa44-12ed47d3ab36" />

<p>&nbsp;</p>

---

## Types of Model Distillation

Model distillation can be carried out in several different ways, each offering a balance between simplicity, effectiveness, and computational cost.  

The most common approaches are ***response distillation*** and ***feature distillation***, which differ in how much of the teacher’s knowledge is exposed to the student.  
  
### 1. Response Distillation
Response distillation is the simplest form of model distillation, where the student learns to mimic the outputs of the teacher model. This can happen at the ***token level***, where the student matches the teacher’s predicted responses directly, or at the ***logit level***, where the student learns from the teacher’s full *probability distribution* over tokens. Both approaches transfer knowledge from the teacher’s outputs, but the logit level provides richer guidance than hard token targets.

<img width="400" height="405" alt="3-kd" src="https://github.com/user-attachments/assets/9bfa7a70-cec8-4c24-89ea-537cc9675f43" />

### a. Token Response Distillation
**Concept:**  
The student model learns directly from the final ***token*** outputs (responses) generated by the teacher.

In ***Token Response Distillation***, also known as token-level or sequence-level distillation, the student model is trained to directly mimic the final token output of the teacher model. For a given input, the teacher model generates a response (e.g., a sequence of text). This output is then used as the target for training the student model. The student's goal is to produce an output that is as close as possible to the teacher's output. This method is straightforward to implement as it only requires the final predictions of the teacher model.

**Advantages:**  
- Simple to implement.  
- Requires no access to internal teacher information (e.g., logits or hidden states).  
- Works well when you only need the student to ***mimic*** end results.  

**Disadvantages:**  
- Provides limited guidance since only the final outputs are used.  
- Less effective at capturing the teacher’s nuanced internl decision boundaries.  

**Complexity:**  
- Easiest and fastest method.  
- Minimal additional infrastructure needed beyond standard supervised learning.  

---

### b. Logit Response Distillation
**Concept:**  
The student is trained on the teacher’s ***probability distribution (soft logits)*** over possible outputs rather than just the hard labels.  

***Logit-level distillation*** involves training the student model to match the ***logits*** of the teacher model. Logits are the raw, unnormalized prediction scores that a model produces before the very final activation function (like softmax) is applied. These logits contain richer information than the final hard predictions, as they reveal the teacher model's ***confidence*** for each possible output. By training on these soft targets, the student model can learn a more ***nuanced understanding*** of the data and often generalize better than if it were trained solely on the final, hard target labels. 

***Temperature*** is often used to *soften* the probability distribution of the teacher's logits, further enhancing the information transferred to the student.

**Advantages:**  
- Encodes richer information about *uncertainty* and relationships between classes.  
- Helps the student generalize better than token response distillation.  
- Higher accuracy with fewer parameters.  

**Disadvantages:**  
- Requires access to teacher logits during training, which can be memory and compute intensive.  
- More sensitive to hyperparameters such as #temperature#.  

**Complexity:**  
- Moderate complexity.  
- Needs additional training pipelines to capture and use logits.  

---

### 3. Feature Distillation
**Concept:**  
The student is trained to match ***internal hidden states or representations*** of the teacher at various layers.

<img width="400" height="417" alt="2-kd-2" src="https://github.com/user-attachments/assets/e3681b62-0854-4cc2-abd8-4d42788c7ead" />

***Feature Distillation***, goes a step further by training the student model to mimic the ***internal representations*** of the teacher model at one or more of its hidden layers. The outputs of these intermediate layers, often referred to as ***embeddings*** or ***feature maps***, capture abstract representations of the input data. By aligning the student's *hidden* layer activations with the teacher's, the student can learn the teacher's feature extraction process. This method allows for a more fine-grained transfer of knowledge, as the student learns not just what the teacher predicts, but also ***how*** it arrives at that prediction by understanding the intermediate data transformations. 

**Advantages:**  
- Transfers internal structural and representational knowledge, not just outputs.  
- Can yield the closest performance to the teacher.  
- Useful when student and teacher architectures are similar.  

**Disadvantages:**  
- Requires significant access to teacher internals.  
- More computationally expensive due to alignment across multiple layers.  
- Risk of over-constraining the student if architectures differ.  

**Complexity:**  
- Highest complexity.  
- Requires careful mapping of teacher layers to student layers.  

---

## Let's Have Qwen3 Teach Gemma3 to do Math!!

<img width="691" height="273" alt="Untitled 2" src="https://github.com/user-attachments/assets/8b9fbc1c-801d-4812-b5f8-bcd13c3bd69c" />

<p>&nbsp;</p>

Here, we demonstrate simple ***Token-level Response Distillation***.

In **distill.py** we use load a 4B parameter **Qwen/Qwen3-4B-Thinking-2507** as the teaching model, as it will output properly-formatted Chain-of-Thought thinking traces before answering.   

We use Google's 1B parameter **Gemma3-1b instruct** model as the smaller student.  It is a tiny 1B model and doesn't emit CoT by default and struggles with tasks like math.


For speed and simplicity, we use the Unsloth.ai python library to perform simple causal supervised fine-tuning (SFT) using the SFTTrainer trainer to teach the student Gemma3:1b model how to reason about math.

<img width="300" height="300" alt="_1d5a5685-2d88-44ca-b50f-ba432cd646ef_9CGCY8lvw4D9JkOdueqsk jpeg copy" src="https://github.com/user-attachments/assets/680498d8-95ff-4529-82e9-0e52e00ec6f0" />

## Method

- We use token-level ***response distillation***
- Load the teacher model (Qwen3)
- Load the student model (Gemma3)
- Load the training data (GSM8K)
- Use Quantized Low-Rank Adaptation (QLoRA) applied to the student (Gemma3)
- The teacher model reads the questions from GSM8K and produces full response
- This creates an interstitial training set:

```
{
  "question": "Mimi picked up 2 dozen seashells...",
  "qwen3_answer": "<think>First Mimi has 24...</think> Therefore, answer is 16."
}
```
- The student is then presented with properly formatted training data from the teacher:

```
messages = [
    {"role": "user", "content": "Question: Mimi picked up 2 dozen seashells..."},
    {"role": "assistant", "content": "<think>First Mimi has 24...</think> Therefore, answer is 16."},
]
```

- We traing for 4 epochs and save QLoRA checkpoints at the end of each epoch
- Given the current batch size and models, this required at least 48GB of GPU RAM.  I used 2 Nvidia RTX 4090 GPUs.

---

# EXAMPLE #1
### Gemma3:1b output ***before*** distillation:

```
QUESTION:
Sarah has 3 boxes of crayons. Each box has 24 crayons. She gives 
10 crayons to her friend. How many crayons does Sarah have left?

MODEL'S ANSWER:
----------------------------------------------------------------------
Let C be the number of crayons in each box. Let N be the number of 
boxes Sarah has. We are given that Sarah has 3 boxes of crayons, 
so N = 3. Each box has 24 crayons, so C = 24. The total number of 
crayons is N × C = 3 × 24 = 72 crayons. Sarah gives 10 crayons to her 
friend. So, the number of crayons Sarah has left is 72 - 10 = 62. 

Final Answer: The final answer is 62.
```

### Gemma3:1b output ***after*** QLoRA response distillation:

```
QUESTION:
Sarah has 3 boxes of crayons. Each box has 24 crayons. She gives 
10 crayons to her friend. How many crayons does Sarah have left?

MODEL'S ANSWER:
----------------------------------------------------------------------
<think>
First, I need to find out how many crayons Sarah has left after giving 
10 crayons to her friend. She starts with 3 boxes of crayons. Each box 
has 24 crayons. So, the total number of crayons she has initially is 
3 boxes * 24 crayons/box = 72 crayons.

Then, she gives 10 crayons to her friend. So, I need to subtract 
10 from the total.

72 - 10 = 62.

Wait, let me double-check. If she has 3 boxes * 24 = 72 crayons. 
Then she gives 10, so 72 - 10 = 62. That seems right.

I don't think I made a mistake here. The problem says "she gives 
10 crayons to her friend," so it's straightforward subtraction.
</think>

Sarah starts with 3 boxes of crayons, each containing 24 crayons.
Total crayons = 3 * 24 = 72 crayons.

She gives 10 crayons to her friend.
Crayons left = 72 - 10 = 62.

The final answer is 62.

62
```



# EXAMPLE #2


```
QUESTION:
At a local café, smoothies are sold in two sizes: a small smoothie
costs 3 dollars and a large smoothie costs 5 dollars. A group of 
friends spends a total of 47 dollars, and they buy twice as many 
small smoothies as large smoothies. How many small smoothies did they 
buy, and what is the average cost per smoothie?


MODEL'S ANSWER:
----------------------------------------------------------------------
Let s be the number of small smoothies bought and l be the number of large smoothies bought.  
The cost of a small smoothie is 3 dollars, and the cost of a large smoothie is 5 dollars.  
The total amount spent is 47 dollars. So, we have the equation:  

3s + 5l = 47  

The group buys twice as many small smoothies as large smoothies, so s = 2l.  
Now we have a system of two equations with two variables:  

3s + 5l = 47  
s = 2l  

Substitute the second equation into the first equation:  

3(2l) + 5l = 47  
6l + 5l = 47  
11l = 47  
l = 47/11 ≈ 4.27  

Since l must be an integer, we can approximate l to be 4. Then s = 2l = 2(4) = 8.  
The cost of 8 small smoothies is 8 × 3 = 24 dollars.  
The cost of 4 large smoothies is 4 × 5 = 20 dollars.  
The total cost is 24 + 20 = 44 dollars.  
However, we want the total cost to be 47 dollars.  

Let s be the number of small smoothies and l be the number of large smoothies.  
We are given that s = 2l.  
The total cost is 3s + 5l = 47.  
Substituting s = 2l, we get:  

3(2l) + 5l = 47  
6l + 5l = 47  
11l = 47  
l = 47/11  

Since l must be an integer, we need to find integers s and l such that s = 2l and 3s + 5l = 47.  
We can rewrite the equation as 3s + 5l = 47.  

If l = 4, then 3s + 5(4) = 47, so 3s + 20 = 47, which means 3s = 27, so s = 9.  
In this case, s = 2l becomes 9 = 2(4), which is 9 = 8, which is not true.  

If l = 5, then 3s + 5(5) = 47, so 3s + 25 = 47, which means 3s = 22, so s = 22/3, which is not an integer.  

Let's try to solve the equation 3s + 5l = 47 with s = 2l.  
Substitute s = 2l into the equation:  

3(2l) + 5l = 47  
6l + 5l = 47  
11l = 47  
l = 47/11 ≈ 4.27  

Since l must be an integer, we can try l = 4. Then s = 2(4) = 8.  
The cost is 3(8) + 5(4) = 24 + 20 = 44.  
We need to add 3 to the cost to get 47.  

So we need to add 3 to the cost of the 8 small smoothies.  
The cost of the 8 small smoothies is 3(8) = 24.  
The cost of the 4 large smoothies is 5(4) = 20.  
The total cost is 24 + 20 = 44.  
We need to add 3 to the total cost, so we need to add 3 to the cost of the 8 small smoothies.  

The cost of the 8 small smoothies is 3s = 3(8) = 24.  
The cost of the 4 large smoothies is 5l = 5(4) = 20.  
The total cost is 24 + 20 = 44.  
We need to add 3 to the total cost, so we need to add 3 to the cost of the 8 small smoothies.  

The cost of the 8 small smoothies is 3s = 3(2l) = 6l.  
The cost of the 4 large smoothies is 5l.  
The total cost is 3s + 5l = 47.  

3(2l) + 5l = 47  
6l + 5l = 47  
11l = 47  
l = 47/11 ≈ 4.27  

We need to find integers s and l such that s = 2l and 3s + 5l = 47.  
If l = 4, then s = 2(4) = 8. The cost is 3(8) + 5(4) = 24 + 20 = 44.  
We need to add 3 to the total cost, so we need to add 3 to the cost of the 8 small smoothies.  

The cost of the 8 small smoothies is 3s = 3(2l) = 6l.  
We want 3s + 5l = 47.  

If s = 2l, then 3(2l) + 5l = 6l + 5l = 11l = 47.  
l = 47/11 ≈ 4.27.  

Let l = 4, then s = 2(4) = 8. The cost is 3(8) + 5(4) = 24 + 20 = 44.  
We need to add 3 to the total cost, so we need to add 3 to the cost of the 8 small smoothies.  

The cost of the 8 small smoothies is 3s = 3(2l) = 6l.  
We want 3s + 5l = 47.  

If s = 2l, then 3(2l) + 5l = 6l + 5l = 11l = 47.  
l = 47/11 ≈ 4.27.  

Let l = 4, then s = 2(4) = 8. The cost is 3(8) + 5(4) = 24 + 20 = 44.  
We need to add 3 to the total cost, so we need to add 3 to the cost of the 8 small smoothies.  

The cost of the 8 small smoothies is 3s = 3(2l) = 6l.  
We want 3s + 5l = 47.  

If s = 2l, then 3(2l) + 5l = 6l + 5l = 11l = 47.  
```












