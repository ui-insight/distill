# distill
Create a mathematical reasoning model through distillation.

<img width="512" height="512" alt="unnamed" src="https://github.com/user-attachments/assets/f9965896-2f3c-44a5-bc78-70a574fb3fd8" />

<p>&nbsp;</p>  

Model distillation is a process where a large ***teacher*** model trains a smaller ***student*** model to replicate its behavior, allowing the student to achieve comparable performance with far fewer parameters. By transferring the teacher’s knowledge, the student retains much of the accuracy while being faster, cheaper, and easier to deploy.

Distilled models can be trained to ***specialize*** in particular tasks or domains such as science, coding, mathematics, and information extraction by inheriting the teacher’s expertise in those areas while remaining compact and efficient for on-prem deployment.

<p>&nbsp;</p>

<img width="768" height="319" alt="knowledge-distillation copy" src="https://github.com/user-attachments/assets/bec18a80-6390-41ff-aa44-12ed47d3ab36" />

<p>&nbsp;</p>

---

## Types of Model Distillation

Model distillation can be carried out in several different ways, each offering a balance between simplicity, effectiveness, and computational cost.  

The most common approaches are ***response distillation*** and ***feature distillation***, which differ in how much of the teacher’s knowledge is exposed to the student.  
  
### 1. Response Distillation
Response distillation is the simplest form of model distillation, where the student learns to mimic the outputs of the teacher model. This can happen at the token level, where the student matches the teacher’s predicted responses directly, or at the logit level, where the student learns from the teacher’s full probability distribution over tokens. Both approaches transfer knowledge from the teacher’s outputs, but the logit level provides richer guidance than hard token targets.

<img width="400" height="405" alt="3-kd" src="https://github.com/user-attachments/assets/9bfa7a70-cec8-4c24-89ea-537cc9675f43" />

### a. Token Response Distillation
**Concept:**  
The student model learns directly from the final ***token*** outputs (responses) generated by the teacher.

In ***response-level distillation***, also known as token-level or sequence-level distillation, the student model is trained to directly mimic the final output of the teacher model. For a given input, the teacher model generates a response (e.g., a sequence of text). This output is then used as the target for training the student model. The student's goal is to produce an output that is as close as possible to the teacher's output. This method is straightforward to implement as it only requires the final predictions of the teacher model.

**Advantages:**  
- Simple to implement.  
- Requires no access to internal teacher information (e.g., logits or hidden states).  
- Works well when you only need the student to ***mimic*** end results.  

**Disadvantages:**  
- Provides limited guidance since only the final outputs are used.  
- Less effective at capturing the teacher’s nuanced decision boundaries.  

**Complexity:**  
- Easiest and fastest method.  
- Minimal additional infrastructure needed beyond standard supervised learning.  

---

### b. Logit Response Distillation
**Concept:**  
The student is trained on the teacher’s ***probability distribution (soft logits)*** over possible outputs rather than just the hard labels.  

***Logit-level distillation*** involves training the student model to match the ***logits*** of the teacher model. Logits are the raw, unnormalized prediction scores that a model produces before the very final activation function (like softmax) is applied. These logits contain richer information than the final hard predictions, as they reveal the teacher model's ***confidence*** for each possible output. By training on these soft targets, the student model can learn a more ***nuanced understanding*** of the data and often generalize better than if it were trained solely on the final, hard target labels. 

***Temperature*** is often used to *soften* the probability distribution of the logits, further enhancing the information transferred to the student.

**Advantages:**  
- Encodes richer information about uncertainty and relationships between classes.  
- Helps the student generalize better than response distillation.  
- Often leads to higher accuracy with fewer parameters.  

**Disadvantages:**  
- Requires access to teacher logits during training, which can be memory and compute intensive.  
- More sensitive to hyperparameters such as temperature.  

**Complexity:**  
- Moderate complexity.  
- Needs additional training pipelines to capture and use logits.  

---

### 3. Feature Distillation
**Concept:**  
The student is trained to match ***internal hidden states or representations*** of the teacher at various layers.

<img width="400" height="417" alt="2-kd-2" src="https://github.com/user-attachments/assets/e3681b62-0854-4cc2-abd8-4d42788c7ead" />

***Layer-wise (aka feature-based) distillation***, goes a step further by training the student model to mimic the ***internal representations*** of the teacher model at one or more of its hidden layers. The outputs of these intermediate layers, often referred to as ***embeddings*** or ***feature maps***, capture abstract representations of the input data. By aligning the student's *hidden* layer activations with the teacher's, the student can learn the teacher's feature extraction process. This method allows for a more fine-grained transfer of knowledge, as the student learns not just what the teacher predicts, but also ***how*** it arrives at that prediction by understanding the intermediate data transformations. 

**Advantages:**  
- Transfers internal structural and representational knowledge, not just outputs.  
- Can yield the closest performance to the teacher.  
- Useful when student and teacher architectures are similar.  

**Disadvantages:**  
- Requires significant access to teacher internals.  
- More computationally expensive due to alignment across multiple layers.  
- Risk of over-constraining the student if architectures differ.  

**Complexity:**  
- Highest complexity.  
- Requires careful mapping of teacher layers to student layers.  
