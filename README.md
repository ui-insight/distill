# distill
Create a mathematical reasoning model through distillation.

<img width="512" height="512" alt="unnamed" src="https://github.com/user-attachments/assets/f9965896-2f3c-44a5-bc78-70a574fb3fd8" />

<p>&nbsp;</p>  

Model distillation is a process where a large ***teacher*** model trains a smaller ***student*** model to replicate its behavior, allowing the student to achieve comparable performance with far fewer parameters. By transferring the teacher’s knowledge, the student retains much of the accuracy while being faster, cheaper, and easier to deploy.

Distilled models can be trained to ***specialize*** in particular tasks or domains such as science, coding, mathematics, and information extraction by inheriting the teacher’s expertise in those areas while remaining compact and efficient for on-prem deployment.

<p>&nbsp;</p>

<img width="768" height="319" alt="knowledge-distillation copy" src="https://github.com/user-attachments/assets/bec18a80-6390-41ff-aa44-12ed47d3ab36" />

<p>&nbsp;</p>

## Types of Model Distillation

Model distillation can be carried out in several different ways, each offering a balance between simplicity, effectiveness, and computational cost.  

The most common approaches are ***response distillation, logit distillation, and layerwise distillation***, which differ in how much of the teacher’s knowledge is exposed to the student.  
  



### 1. Response Distillation
**Concept:**  
The student model learns directly from the final outputs (responses) generated by the teacher, such as predicted tokens or labels.  

**Advantages:**  
- Simple to implement.  
- Requires no access to intermediate teacher information (e.g., logits or hidden states).  
- Works well when you only need the student to mimic end results.  

**Disadvantages:**  
- Provides limited guidance since only the final outputs are used.  
- Less effective at capturing the teacher’s nuanced decision boundaries.  

**Complexity:**  
- Easiest and fastest method.  
- Minimal additional infrastructure needed beyond standard supervised learning.  

---

### 2. Logit Distillation
**Concept:**  
The student is trained on the teacher’s probability distribution (soft logits) over possible outputs rather than just the hard labels.  

**Advantages:**  
- Encodes richer information about uncertainty and relationships between classes.  
- Helps the student generalize better than response distillation.  
- Often leads to higher accuracy with fewer parameters.  

**Disadvantages:**  
- Requires access to teacher logits during training, which can be memory and compute intensive.  
- More sensitive to hyperparameters such as temperature.  

**Complexity:**  
- Moderate complexity.  
- Needs additional training pipelines to capture and use logits.  

---

### 3. Layerwise Distillation
**Concept:**  
The student is trained to match intermediate hidden states or representations of the teacher at various layers.  

**Advantages:**  
- Transfers structural and representational knowledge, not just outputs.  
- Can yield the closest performance to the teacher.  
- Useful when student and teacher architectures are similar.  

**Disadvantages:**  
- Requires significant access to teacher internals.  
- More computationally expensive due to alignment across multiple layers.  
- Risk of over-constraining the student if architectures differ.  

**Complexity:**  
- Highest complexity.  
- Requires careful mapping of teacher layers to student layers.  
